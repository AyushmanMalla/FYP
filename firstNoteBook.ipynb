{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test code to see xrv lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[202, 199, 195, ...,   5,   2,   0],\n",
       "       [199, 196, 195, ...,   5,   2,   0],\n",
       "       [196, 194, 193, ...,   5,   2,   0],\n",
       "       ...,\n",
       "       [255, 255, 255, ...,   0,   0,   0],\n",
       "       [255, 255, 254, ...,   0,   0,   0],\n",
       "       [255, 255, 255, ...,   0,   0,   0]],\n",
       "      shape=(1024, 1024), dtype=uint8)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = skimage.io.imread(r\"C:\\Users\\Ayushman\\OneDrive - Nanyang Technological University\\FYP\\NIH_CXR_Dataset\\CXR_ALL_FLAT\\00000001_000.png\")\n",
    "xrv.datasets.normalize(img, 255)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Atelectasis': np.float32(0.57338643),\n",
       " 'Consolidation': np.float32(0.5580266),\n",
       " 'Infiltration': np.float32(0.5303836),\n",
       " 'Pneumothorax': np.float32(0.5043328),\n",
       " 'Edema': np.float32(0.10917771),\n",
       " 'Emphysema': np.float32(0.5009655),\n",
       " 'Fibrosis': np.float32(0.54985535),\n",
       " 'Effusion': np.float32(0.6399379),\n",
       " 'Pneumonia': np.float32(0.09603219),\n",
       " 'Pleural_Thickening': np.float32(0.534311),\n",
       " 'Cardiomegaly': np.float32(0.3065372),\n",
       " 'Nodule': np.float32(0.5630389),\n",
       " 'Mass': np.float32(0.5961195),\n",
       " 'Hernia': np.float32(0.0095163295),\n",
       " 'Lung Lesion': np.float32(0.6115719),\n",
       " 'Fracture': np.float32(0.5213179),\n",
       " 'Lung Opacity': np.float32(0.7580718),\n",
       " 'Enlarged Cardiomediastinum': np.float32(0.56434613)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchxrayvision as xrv\n",
    "import skimage, torch, torchvision\n",
    "\n",
    "# Prepare the image:\n",
    "img = skimage.io.imread(r\"C:\\Users\\Ayushman\\OneDrive - Nanyang Technological University\\FYP\\NIH_CXR_Dataset\\CXR_ALL_FLAT\\00021091_002.png\")\n",
    "img = xrv.datasets.normalize(img, 255) # convert 8-bit image to [-1024, 1024] range\n",
    "# img = img.mean(2)[None, ...] # Make single color channel\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    xrv.datasets.XRayCenterCrop(),\n",
    "    xrv.datasets.XRayResizer(224),\n",
    "])\n",
    "\n",
    "\n",
    "img = img[None, :, :]\n",
    "img = transform(img)\n",
    "img = torch.from_numpy(img)\n",
    "\n",
    "# Load model and process image\n",
    "model = xrv.models.DenseNet(weights=\"densenet121-res224-all\")\n",
    "outputs = model(img[None,...]) # or model.features(img[None,...])\n",
    "\n",
    "# Print results\n",
    "dict(zip(model.pathologies, outputs[0].detach().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5845, 0.5440, 0.5108, 0.5221, 0.5296, 0.5219, 0.5100, 0.5407, 0.6170,\n",
       "         0.5343, 0.5324, 0.5211, 0.5590, 0.2375, 0.2073, 0.5179, 0.6812, 0.5643]],\n",
       "       grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load NIH Dataset - only PA view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Atelectasis': {np.float32(0.0): 61582, np.float32(1.0): 5728},\n",
      " 'Cardiomegaly': {np.float32(0.0): 65747, np.float32(1.0): 1563},\n",
      " 'Consolidation': {np.float32(0.0): 65789, np.float32(1.0): 1521},\n",
      " 'Edema': {np.float32(0.0): 67034, np.float32(1.0): 276},\n",
      " 'Effusion': {np.float32(0.0): 60721, np.float32(1.0): 6589},\n",
      " 'Emphysema': {np.float32(0.0): 65811, np.float32(1.0): 1499},\n",
      " 'Fibrosis': {np.float32(0.0): 65902, np.float32(1.0): 1408},\n",
      " 'Hernia': {np.float32(0.0): 67118, np.float32(1.0): 192},\n",
      " 'Infiltration': {np.float32(0.0): 57957, np.float32(1.0): 9353},\n",
      " 'Mass': {np.float32(0.0): 63743, np.float32(1.0): 3567},\n",
      " 'Nodule': {np.float32(0.0): 63133, np.float32(1.0): 4177},\n",
      " 'Pleural_Thickening': {np.float32(0.0): 64892, np.float32(1.0): 2418},\n",
      " 'Pneumonia': {np.float32(0.0): 66680, np.float32(1.0): 630},\n",
      " 'Pneumothorax': {np.float32(0.0): 63903, np.float32(1.0): 3407}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NIH_Dataset num_samples=67310 views=['PA'] data_aug=None"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nih_dataset = xrv.datasets.NIH_Dataset(imgpath=r\"C:\\Users\\Ayushman\\OneDrive - Nanyang Technological University\\FYP\\NIH_CXR_Dataset\\CXR_ALL_FLAT\", \n",
    "                         csvpath=r\"C:\\Users\\Ayushman\\OneDrive - Nanyang Technological University\\FYP\\NIH_CXR_Dataset\\Data_Entry_2017.csv\",\n",
    "                         bbox_list_path=r\"C:\\Users\\Ayushman\\OneDrive - Nanyang Technological University\\FYP\\NIH_CXR_Dataset\\BBox_List_2017.csv\", \n",
    "                         views=[\"PA\"],\n",
    "                         unique_patients=False)\n",
    "nih_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced subset created with 988 unique images across 5 classes.\n",
      "\n",
      "Class distribution in sampled subset:\n",
      "Pneumonia: 200 samples\n",
      "Effusion: 200 samples\n",
      "Cardiomegaly: 200 samples\n",
      "Infiltration: 200 samples\n",
      "Atelectasis: 200 samples\n",
      "\n",
      "Metadata CSV saved to: C:/Users/Ayushman/OneDrive - Nanyang Technological University/FYP/NIH_CXR_Dataset\\sampled_subset_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import torchxrayvision as xrv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Optional: Merge all NIH image subfolders into one flat directory\n",
    "src_base = \"C:/Users/Ayushman/OneDrive - Nanyang Technological University/FYP/NIH_CXR_Dataset\"\n",
    "\n",
    "# Load the NIH metadata using pandas\n",
    "csv_path = os.path.join(src_base, \"Data_Entry_2017.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Filter for PA view only\n",
    "df = df[df[\"View Position\"] == \"PA\"]\n",
    "\n",
    "# Define target conditions\n",
    "target_conditions = [\"Pneumonia\", \"Effusion\", \"Cardiomegaly\", \"Infiltration\", \"Atelectasis\"]\n",
    "\n",
    "# Create binary columns for each target condition\n",
    "def label_contains(label_str, condition):\n",
    "    return int(condition in str(label_str).split(\"|\"))\n",
    "\n",
    "for condition in target_conditions:\n",
    "    df[condition] = df[\"Finding Labels\"].apply(lambda x: label_contains(x, condition))\n",
    "\n",
    "# Select up to N samples per condition\n",
    "samples_per_class = 200\n",
    "selected_dfs = []\n",
    "class_distribution = {}\n",
    "\n",
    "for condition in target_conditions:\n",
    "    pos_samples = df[df[condition] == 1]\n",
    "    selected = pos_samples.sample(n=min(samples_per_class, len(pos_samples)), random_state=42)\n",
    "    selected_dfs.append(selected)\n",
    "    class_distribution[condition] = len(selected)\n",
    "\n",
    "# Combine and deduplicate\n",
    "selected_df = pd.concat(selected_dfs).drop_duplicates(subset=\"Image Index\")\n",
    "selected_df = selected_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Balanced subset created with {len(selected_df)} unique images across {len(target_conditions)} classes.\")\n",
    "print(\"\\nClass distribution in sampled subset:\")\n",
    "for condition, count in class_distribution.items():\n",
    "    print(f\"{condition}: {count} samples\")\n",
    "\n",
    "# Save selected metadata to CSV\n",
    "subset_csv_path = os.path.join(src_base, \"sampled_subset_metadata.csv\")\n",
    "selected_df.to_csv(subset_csv_path, index=False)\n",
    "print(f\"\\nMetadata CSV saved to: {subset_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 25/988 [00:29<17:49,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00004808_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 175/988 [03:31<15:28,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00014211_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 328/988 [06:32<12:49,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00000084_000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 451/988 [08:55<10:40,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00001369_000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 607/988 [11:58<07:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00004750_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 780/988 [15:18<04:01,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00004281_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 802/988 [15:44<03:59,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00000955_002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 919/988 [18:01<01:21,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00007156_000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 988/988 [19:24<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Results saved to inference_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchxrayvision as xrv\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config\n",
    "CSV_PATH = \"C:/Users/Ayushman/OneDrive - Nanyang Technological University/FYP/NIH_CXR_Dataset/sampled_subset_metadata.csv\"\n",
    "IMAGE_DIR = r\"C:\\Users\\Ayushman\\OneDrive - Nanyang Technological University\\FYP\\NIH_CXR_Dataset\\CXR_ALL_FLAT\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Load pretrained model\n",
    "model = xrv.models.DenseNet(weights=\"densenet121-res224-all\")\n",
    "# model = model.to(DEVICE)\n",
    "# model.eval()\n",
    "\n",
    "# Define image preprocessing\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor()  # for grayscale images\n",
    "# ])\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    xrv.datasets.XRayCenterCrop(),\n",
    "    xrv.datasets.XRayResizer(224),\n",
    "])\n",
    "\n",
    "\n",
    "# Map torchxrayvision class indices to disease names\n",
    "disease_labels = model.pathologies\n",
    "\n",
    "# Prepare results\n",
    "results = []\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            img_path = os.path.join(IMAGE_DIR, row[\"Image Index\"])  # Adjust column name if needed\n",
    "            img = skimage.io.imread(img_path)\n",
    "            img = img[None, :, :]\n",
    "            img = transform(img)\n",
    "            img = torch.from_numpy(img)\n",
    "            # img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "            preds = model(img[None, ...])\n",
    "            preds_np = preds.cpu().numpy().flatten()\n",
    "\n",
    "            # Store results\n",
    "            pred_dict = {\"image_filename\": row[\"Image Index\"]}\n",
    "            pred_dict.update({disease_labels[i]: preds_np[i] for i in range(len(disease_labels))})\n",
    "            results.append(pred_dict)\n",
    "        except:\n",
    "            print(\"Could not run inference for: \", row[\"Image Index\"])\n",
    "\n",
    "# Save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"inference_results.csv\", index=False)\n",
    "print(\"Inference complete. Results saved to inference_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 25/988 [00:15<10:28,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00004808_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 175/988 [02:07<09:06,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00014211_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 328/988 [04:02<06:30,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00000084_000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 451/988 [05:21<05:04,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00001369_000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 607/988 [07:02<05:41,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00004750_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 780/988 [08:46<02:22,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00004281_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 802/988 [09:01<02:54,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00000955_002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 919/988 [10:25<00:47,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00007156_000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 988/988 [11:04<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Results saved to inference_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchxrayvision as xrv\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config\n",
    "CSV_PATH = \"C:/Users/Ayushman/OneDrive - Nanyang Technological University/FYP/NIH_CXR_Dataset/sampled_subset_metadata.csv\"\n",
    "IMAGE_DIR = r\"C:\\Users\\Ayushman\\OneDrive - Nanyang Technological University\\FYP\\NIH_CXR_Dataset\\CXR_ALL_FLAT\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Load pretrained model\n",
    "model = xrv.models.DenseNet(weights=\"densenet121-res224-chex\")\n",
    "# model = model.to(DEVICE)\n",
    "# model.eval()\n",
    "\n",
    "# Define image preprocessing\n",
    "transform = torchvision.transforms.Compose([\n",
    "    xrv.datasets.XRayCenterCrop(),\n",
    "    xrv.datasets.XRayResizer(224),\n",
    "])\n",
    "\n",
    "\n",
    "# Map torchxrayvision class indices to disease names\n",
    "disease_labels = model.pathologies\n",
    "\n",
    "# Prepare results\n",
    "results = []\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            img_path = os.path.join(IMAGE_DIR, row[\"Image Index\"])  # Adjust column name if needed\n",
    "            img = skimage.io.imread(img_path)\n",
    "            img = img[None, :, :]\n",
    "            img = transform(img)\n",
    "            img = torch.from_numpy(img)\n",
    "            # img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "            preds = model(img[None, ...])\n",
    "            preds_np = preds.cpu().numpy().flatten()\n",
    "\n",
    "            # Store results\n",
    "            pred_dict = {\"image_filename\": row[\"Image Index\"]}\n",
    "            pred_dict.update({disease_labels[i]: preds_np[i] for i in range(len(disease_labels))})\n",
    "            results.append(pred_dict)\n",
    "        except:\n",
    "            print(\"Could not run inference for: \", row[\"Image Index\"])\n",
    "\n",
    "# Save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"inference_results_chex.csv\", index=False)\n",
    "print(\"Inference complete. Results saved to inference_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 25/988 [00:06<03:59,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00004808_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 175/988 [03:19<24:11,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00014211_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 328/988 [06:56<14:17,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00000084_000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 451/988 [10:25<19:37,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00001369_000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 607/988 [15:52<13:10,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00004750_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 780/988 [22:00<07:16,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00004281_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 802/988 [22:45<06:37,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00000955_002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 919/988 [26:55<02:28,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00007156_000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 988/988 [29:22<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Results saved to inference_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchxrayvision as xrv\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config\n",
    "CSV_PATH = \"C:/Users/Ayushman/OneDrive - Nanyang Technological University/FYP/NIH_CXR_Dataset/sampled_subset_metadata.csv\"\n",
    "IMAGE_DIR = r\"C:\\Users\\Ayushman\\OneDrive - Nanyang Technological University\\FYP\\NIH_CXR_Dataset\\CXR_ALL_FLAT\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Load pretrained model\n",
    "model = xrv.models.DenseNet(weights=\"densenet121-res224-mimic_nb\")\n",
    "# model = model.to(DEVICE)\n",
    "# model.eval()\n",
    "\n",
    "# Define image preprocessing\n",
    "transform = torchvision.transforms.Compose([\n",
    "    xrv.datasets.XRayCenterCrop(),\n",
    "    xrv.datasets.XRayResizer(224),\n",
    "])\n",
    "\n",
    "\n",
    "# Map torchxrayvision class indices to disease names\n",
    "disease_labels = model.pathologies\n",
    "\n",
    "# Prepare results\n",
    "results = []\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            img_path = os.path.join(IMAGE_DIR, row[\"Image Index\"])  # Adjust column name if needed\n",
    "            img = skimage.io.imread(img_path)\n",
    "            img = img[None, :, :]\n",
    "            img = transform(img)\n",
    "            img = torch.from_numpy(img)\n",
    "            # img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "            preds = model(img[None, ...])\n",
    "            preds_np = preds.cpu().numpy().flatten()\n",
    "\n",
    "            # Store results\n",
    "            pred_dict = {\"image_filename\": row[\"Image Index\"]}\n",
    "            pred_dict.update({disease_labels[i]: preds_np[i] for i in range(len(disease_labels))})\n",
    "            results.append(pred_dict)\n",
    "        except:\n",
    "            print(\"Could not run inference for: \", row[\"Image Index\"])\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"inference_results_mimic_nb.csv\", index=False)\n",
    "print(\"Inference complete. Results saved to inference_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 25/988 [00:12<08:27,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00004808_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 175/988 [01:32<07:32,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00014211_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 328/988 [02:57<07:21,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00000084_000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 451/988 [04:04<05:16,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00001369_000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 607/988 [05:37<04:02,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00004750_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 780/988 [07:13<02:23,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00004281_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 802/988 [07:25<01:43,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00000955_002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 919/988 [08:34<00:40,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not run inference for:  00007156_000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 988/988 [09:13<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Results saved to inference_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchxrayvision as xrv\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config\n",
    "CSV_PATH = \"C:/Users/Ayushman/OneDrive - Nanyang Technological University/FYP/NIH_CXR_Dataset/sampled_subset_metadata.csv\"\n",
    "IMAGE_DIR = r\"C:\\Users\\Ayushman\\OneDrive - Nanyang Technological University\\FYP\\NIH_CXR_Dataset\\CXR_ALL_FLAT\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Load pretrained model\n",
    "model = xrv.models.DenseNet(weights=\"densenet121-res224-nih\")\n",
    "# model = model.to(DEVICE)\n",
    "# model.eval()\n",
    "\n",
    "# Define image preprocessing\n",
    "transform = torchvision.transforms.Compose([\n",
    "    xrv.datasets.XRayCenterCrop(),\n",
    "    xrv.datasets.XRayResizer(224),\n",
    "])\n",
    "\n",
    "\n",
    "# Map torchxrayvision class indices to disease names\n",
    "disease_labels = model.pathologies\n",
    "\n",
    "# Prepare results\n",
    "results = []\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            img_path = os.path.join(IMAGE_DIR, row[\"Image Index\"])  # Adjust column name if needed\n",
    "            img = skimage.io.imread(img_path)\n",
    "            img = img[None, :, :]\n",
    "            img = transform(img)\n",
    "            img = torch.from_numpy(img)\n",
    "            # img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "            preds = model(img[None, ...])\n",
    "            preds_np = preds.cpu().numpy().flatten()\n",
    "\n",
    "            # Store results\n",
    "            pred_dict = {\"image_filename\": row[\"Image Index\"]}\n",
    "            pred_dict.update({disease_labels[i]: preds_np[i] for i in range(len(disease_labels))})\n",
    "            results.append(pred_dict)\n",
    "        except:\n",
    "            print(\"Could not run inference for: \", row[\"Image Index\"])\n",
    "# Save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"inference_results_nih.csv\", index=False)\n",
    "print(\"Inference complete. Results saved to inference_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Atelectasis',\n",
       " 'Consolidation',\n",
       " '',\n",
       " 'Pneumothorax',\n",
       " 'Edema',\n",
       " '',\n",
       " '',\n",
       " 'Effusion',\n",
       " 'Pneumonia',\n",
       " '',\n",
       " 'Cardiomegaly',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Lung Lesion',\n",
       " 'Fracture',\n",
       " 'Lung Opacity',\n",
       " 'Enlarged Cardiomediastinum']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xrv.models.DenseNet(weights=\"densenet121-res224-mimic_nb\") #Cchex , mimic_nb, resnet50-res512-chex, resnet50-res512-mimic_ch\n",
    "model.pathologies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "targets = [\"Pneumonia\", \"Effusion\", \"Cardiomegaly\", \"Infiltration\", \"Atelectasis\"]\n",
    "\n",
    "model_csvs = {\n",
    "    \"NIH\": \"inference_results_nih.csv\",\n",
    "    \"CheXpert\": \"inference_results_chex.csv\",\n",
    "    \"MIMIC\": \"inference_results_mimic_nb.csv\",\n",
    "    \"All\": \"inference_results.csv\"\n",
    "}\n",
    "\n",
    "summary = {}\n",
    "\n",
    "for model_name, path in model_csvs.items():\n",
    "    df = pd.read_csv(path)\n",
    "    summary[model_name] = [df[label].mean() for label in targets]\n",
    "\n",
    "summary_df = pd.DataFrame(summary, index=targets)\n",
    "print(summary_df)\n",
    "summary_df.to_csv(\"model_mean_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
